{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Statistique en Bioinformatique : </b> TME5 et 6 </h1>\n",
    "<br>\n",
    "L’objectif de ce TME est: \n",
    "<br>\n",
    "<ul>\n",
    "<li> implémenter l'algorithme de Viterbi et l'estimation des paramètres (en utilisant le Viterbi training)\n",
    "pour l'exemple du occasionally dishonest casino.   </li> \n",
    "</ul>\n",
    "<br>\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n",
    "<p>**Soumission**</p>\n",
    "<ul>\n",
    "<li>Renomer le fichier TME5_6.ipynb pour NomEtudiant1_NomEtudiant2.ipynb </li>\n",
    "<li>Envoyer par email à edoardo.sarti@upmc.fr, l’objet du email sera [SBAS-2019] TME5 (deadline 19/03/2018 23:59)</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nom etudiant 1 : Alex YE\n",
    "<br>\n",
    "Nom etudiant 2 :\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Introduction</h3>\n",
    "Un casino parfois malhonnête (occasionally dishonest casino) utilise 2 types de pieces : fair et unfair. <br>\n",
    "La matrice de transition entre les états cachés est:<br>\n",
    "${\\cal S}=\\{F,U\\}$ (fair, unfair):\n",
    "$$\n",
    "p = \\left(\n",
    "\\begin{array}{cc}\n",
    "0.99 & 0.01\\\\\n",
    "0.05 & 0.95\n",
    "\\end{array}\n",
    "\\right)\\ ,\n",
    "$$\n",
    "\n",
    "les probabilités d'éemission des symboles \n",
    "${\\cal O} = \\{H,T\\}$ (head, tail):\n",
    "\\begin{eqnarray}\n",
    "e_F(H) =  0.5 &\\ \\ \\ \\ &\n",
    "e_F(T) = 0.5 \\nonumber\\\\\n",
    "e_U(H) = 0.9 &\\ \\ \\ \\ &\n",
    "e_U(T) = 0.1 \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "<br> Et la condition initiale $\\pi^{(0)} = (1,0)$ (le jeux commence toujours avec le pieces juste (fair)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercice 1</b>:\n",
    "<u>Simulation</u>: Ecrire une fonction qui simule $T$ jets de pieces. \n",
    "La fonction renverra un tableau à deux colonnes correspondant \n",
    "aux valeurs simulées pour les états cachés $X_t$ \n",
    "(type de dés utilisée, “F” ou “U”) et aux symboles observées $Y_t$ \n",
    "(résultat du jet de dés, “H” ou “T”). On simulera une séquence\n",
    "de longueur 2000 qu'on gardera pour les applications ultérieures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "S = { 0:'F',1 :'U'}\n",
    "Pij = np.array([[0.99, 0.01], \n",
    "                [0.05, 0.95]])\n",
    "\n",
    "O = {0:'H', 1: 'T'}\n",
    "Eij = np.array([[0.5, 0.5], \n",
    "                [0.9, 0.1]]) #ça aurait dû être Eio\n",
    "\n",
    "# Condition initiale\n",
    "pi0=np.array([0.999,0.001])\n",
    "\n",
    "T = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 1 0 0]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Fonction qui simule T jets de pieces\n",
    "def toss(times, pi, trans, emiss):\n",
    "    #il n'y a toujours que 2 états possibles, si ce n'est pas le premier c'est forcément l'autre. \n",
    "    hid = np.random.rand(times)\n",
    "    obs = np.random.rand(times)\n",
    "    state = []\n",
    "    \n",
    "    #initial state\n",
    "    curr_state = 0\n",
    "    if(hid[0] < pi[0]):\n",
    "        state.append(0)\n",
    "        curr_state = 0\n",
    "    else:\n",
    "        state.append(1)\n",
    "        curr_state = 1\n",
    "        \n",
    "    #state transition\n",
    "    for i in range(1,times):\n",
    "        if(hid[i] < trans[curr_state][0]):\n",
    "            state.append(0)\n",
    "            curr_state = 0\n",
    "        else:\n",
    "            state.append(1)\n",
    "            curr_state = 1\n",
    "    \n",
    "    #emission from state\n",
    "    tir = []\n",
    "    for i in range(times):\n",
    "        if(obs[i]< emiss[state[i]][0]):\n",
    "            tir.append(0) #on ne vas pas mettre F ou U, pour simplifier les calculs suivants.\n",
    "        else:\n",
    "            tir.append(1)\n",
    "    \n",
    "    return np.array(state), np.array(tir)\n",
    "    \n",
    "states, sample = toss(T, pi0, Pij, Eij)\n",
    "print(sample)\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercice 2</b>: <u>Algorithme de Viterbi </u>: Ecrire une fonction qui permet\n",
    "de déterminer la séquence $(i^\\star_t)_{t=0:T}$ d'états cachés\n",
    "plus probable, ansi que sa probabilité. Pour tester votre fonction utiliser le résultat de la \n",
    "simulation (2éme colonne) de la question 1. Comparer $(i^\\star_t)_{t=0:T}$ avec\n",
    "les vrais états cachés (1ère colonne de la simulation). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "0.0\n",
      "0.652\n"
     ]
    }
   ],
   "source": [
    "# unused \n",
    "def forward(seq, trans, emiss):    \n",
    "    alpha = np.zeros((seq.size,len(emiss)))\n",
    "    alpha[0][:] = 1\n",
    "    for i in range(len(trans)): #for all state\n",
    "        for t in range(1,seq.size): #for all observations\n",
    "            #there's an alpha that equals\n",
    "            alpha[t][i] = emiss[i][seq[t]] * sum(alpha[i-1][:]*trans[:][i]) \n",
    "    return alpha\n",
    "\n",
    "#not working\n",
    "def backward(seq, trans, emiss):\n",
    "    beta = np.zeros((seq.size,len(emiss)))\n",
    "    beta[seq.size-1][:] = 1\n",
    "    for i in range(len(trans)): #for all state\n",
    "        for t in range(seq.size-1-1,0,-1): #for all observations\n",
    "            beta[t-1][i] = sum( trans[i][:] * emiss[:][seq[t]] * beta[t][:] )\n",
    "            print(beta[t-1][i])\n",
    "    return beta\n",
    "\n",
    "\n",
    "from decimal import Decimal, getcontext\n",
    "getcontext().prec = 30\n",
    "\n",
    "def max_pointer(v, trans):\n",
    "    choices = []\n",
    "    for pji in trans:\n",
    "        choices.append(v*pji)\n",
    "    choices = np.array(choices)\n",
    "    return np.argmax(choices)\n",
    "\n",
    "# Algorithme de Viterbi\n",
    "def viterbi(seq, pi, trans, emiss):    \n",
    "    best = np.zeros(seq.size)\n",
    "    options = np.zeros(len(emiss))\n",
    "    vit = np.zeros((seq.size,len(emiss)))\n",
    "    vit[0][:] = 1\n",
    "    for t in range(1,seq.size): #for all observations\n",
    "        for i in range(len(trans)): #for all state\n",
    "            #there's a value that equals\n",
    "            \n",
    "            #best_j = np.max(vit[t-1][i] * trans[:][i])\n",
    "            #vit[t][i] = emiss[i][seq[t]] * best_j \n",
    "            \n",
    "            best_j = np.max(vit[t-1][i] * trans[:][i])\n",
    "            vit[t][i] = emiss[i][seq[t]] * best_j \n",
    "            \n",
    "            ##avoid log of negative number\n",
    "            #replace = False\n",
    "            #best_j = np.max(vit[t-1][i] * trans[:][i])\n",
    "            #if(best_j < 0):\n",
    "            #    best_j = -best_j\n",
    "            #    replace = True\n",
    "            #vit[t][i] = np.log( emiss[i][seq[t]] * best_j )\n",
    "            #if(replace):\n",
    "            #    vit[t][i] = -vit[t][i]\n",
    "            \n",
    "            \n",
    "            options[i] = np.argmax(vit[t-1][i] * trans[:][i])\n",
    "            #options[i] = max_pointer(vit, trans[i])\n",
    "            \n",
    "            #print(vit[t][i])\n",
    "            \n",
    "        best[t] = np.argmax(options)\n",
    "    #print(best)\n",
    "    return np.array(best) , vit\n",
    "\n",
    "'''\n",
    "def prob_path( path, pi, trans):    #######prob avec viterbi\n",
    "    state = int(path[0])\n",
    "    prob = pi[state]\n",
    "    for i in range(1,path.size):\n",
    "        next_ = int(path[i])\n",
    "        prob *= trans[state][next_]\n",
    "        state = next_\n",
    "    return prob\n",
    "'''\n",
    "\n",
    "def prob_path(l):\n",
    "    log_odds = np.exp( l )\n",
    "    log_odds = log_odds / log_odds.sum()\n",
    "    return np.prod(log_odds)\n",
    "\n",
    "path = viterbi(sample, pi0, Pij, Eij)\n",
    "path, l_probas = viterbi(sample, pi0, Pij, Eij)\n",
    "prob = prob_path( np.array(l_probas) )\n",
    "\n",
    "print(path)\n",
    "print(prob) #très peu probable\n",
    "\n",
    "#on compare et on compte tous les états cachés identiques.\n",
    "same = np.where(states==path, 1, 0)\n",
    "\n",
    "print(same.sum()/path.size) # ~60% correct "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercice 3</b>: <u>Estimation des paramètres</u>\n",
    "<br>\n",
    "3.1) Ecrire une fonction qui utilise tous les résultats de la simulation\n",
    "(états et symboles) pour compter les nombres d'occurrence $N_{ij}$ est $M_{iO}$ définis\n",
    "en cours. Estimer $p_{ij}$ est $e_i(O)$, voir slides  37-39 dans la presentation. Attention, pour eviter les probabilites à zero nous alons utiliser les pseudo-count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99288061 0.00711939]\n",
      " [0.07344633 0.92655367]]\n",
      "[[0.54553991 0.45446009]\n",
      " [0.51599147 0.48400853]]\n"
     ]
    }
   ],
   "source": [
    "template = np.array([[ \"00\", \"01\"],[ \"10\", \"11\"]])\n",
    "\n",
    "# Estimation de Parametres par contage\n",
    "def counting(seq, char):\n",
    "    #pseudo count\n",
    "    comb_of_interest = np.ones( template[char].size )\n",
    "    prev = None\n",
    "    for c in seq:\n",
    "        if(prev == char):\n",
    "            key = str(prev)+str(c)\n",
    "            ind = np.where(template[char] == key)\n",
    "            comb_of_interest[ind] += 1\n",
    "        prev = c\n",
    "    return comb_of_interest/comb_of_interest.sum()\n",
    "\n",
    "\n",
    "def estimate(seq):\n",
    "    #we know there's exactly 2 options\n",
    "    count1 = counting(seq, 0)\n",
    "    count2 = counting(seq, 1)\n",
    "    \n",
    "    return np.vstack((count1, count2))\n",
    "    \n",
    "    \n",
    "#the sample haven't been transformed to characters.\n",
    "hid_estimate = estimate(states)\n",
    "obs_estimate = estimate(sample)\n",
    "\n",
    "print(hid_estimate)\n",
    "print(obs_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2) <u> Viterbi training </u>: Ecrire une fonction qui utilise \n",
    "seulement la séquence $(O_t)_{t=0:T}$ (2emme colone de la simulation) pour estimer les \n",
    "paramètres $p_{ij}$ est $e_i(O)$. On s'arretera quand les diferences entre les logVraissamblance est inferieur à 1e-04. Comparer les résultats de 3.1 et de 3.2 (3.2 avec plusieurs restarts,\n",
    "et avec initialisation des paramètres alèatoire).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.84677419 0.15322581]\n",
      " [0.99625468 0.00374532]]\n",
      "[[0.5682243  0.4317757 ]\n",
      " [0.49624866 0.50375134]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "delta = 1e-04\n",
    "\n",
    "# Initialisation aleatoire de Pij, Eij, pi0\n",
    "def rand_param():\n",
    "    Pij = []\n",
    "    one = random.random()\n",
    "    two = 1 - one\n",
    "    Pi = [one, two]\n",
    "    Pij.append(Pi)\n",
    "    one = random.random()\n",
    "    two = 1 - one\n",
    "    Pi = [one, two]\n",
    "    Pij.append(Pi)\n",
    "    Pij = np.array(Pij)\n",
    "\n",
    "    Eij = []\n",
    "    one = random.random()\n",
    "    two = 1 - one\n",
    "    Ei = [one, two]\n",
    "    Eij.append(Ei)\n",
    "    one = random.random()\n",
    "    two = 1 - one\n",
    "    Ei = [one, two]\n",
    "    Eij.append(Ei)\n",
    "    Eij = np.array(Eij)\n",
    "\n",
    "    one = random.random()\n",
    "    two = 1 - one\n",
    "    pi0 = [one, two]\n",
    "    \n",
    "    return pi0, Pij, Eij\n",
    "\n",
    "# Calcule log Vraissamblance\n",
    "def log_likelihood( sample, path, pi, trans, emiss):\n",
    "    state = int(path[0])\n",
    "    obs = sample[0]\n",
    "    prob = np.log(pi[state] * emiss[state][obs])\n",
    "    for i in range(1,sample.size):\n",
    "        obs = sample[i]\n",
    "        next_ = int(path[i])\n",
    "        prob += np.log(trans[state][next_] * emiss[state][obs])\n",
    "        state = next_\n",
    "    return prob\n",
    "\n",
    "# Viterbi Training\n",
    "def viterbi_train(sample, pi, trans, emiss, tresh = delta, max_iter = 50):\n",
    "    #first iter to get a log likelihood\n",
    "    path, devnull = viterbi(sample, pi, trans, emiss)\n",
    "    previous_step = log_likelihood(sample, path, pi, trans, emiss)\n",
    "    \n",
    "    path = np.array(list(map(int, path)))\n",
    "    Pij = estimate(path)\n",
    "    Eij = estimate(sample) # won't and shouldn't be modified afterward.     \n",
    "    \n",
    "    #iteration\n",
    "    for i in range(max_iter):\n",
    "        #print(i)\n",
    "        \n",
    "        path, devnull = viterbi(sample, pi, trans, emiss)\n",
    "        like = log_likelihood(sample, path, pi, Pij, Eij)\n",
    "        \n",
    "        #if we are close enough we stop\n",
    "        if(abs(abs(previous_step)-abs(like)) <= tresh):\n",
    "            return Pij, Eij\n",
    "        previous_step = like \n",
    "        \n",
    "        path = np.array(list(map(int, path)))\n",
    "        Pij = estimate(path)\n",
    "        #Eij = estimate(sample) # The sample won't and shouldn't be modified.     \n",
    "    return Pij, Eij\n",
    "\n",
    "pi0, Pij, Eij = rand_param()\n",
    "Pij, Eij = viterbi_train(sample, pi0, Pij, Eij)\n",
    "print()\n",
    "print(Pij)\n",
    "print(Eij)\n",
    "\n",
    "#On est vraiment très proche des valeurs obtenues par counting,\n",
    "#Mais par moment subit de grande variation\n",
    "\n",
    "#Met toujours 2 itérations ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3) <u>Viterbi training deuxiemme version </u> Ecrivez une version de 3.3 qui:\n",
    "- part plusieurs fois (100x) d'une initialisation aléatoire des \n",
    "paramètres de l'HMM,\n",
    "- utilise Viterbi training pour estimer les paramètres,\n",
    "- calcule la log-vraisemblance pour les paramètres estimés,\n",
    "- sauvegarde seulement l'estimation avec la valeur maximale de la\n",
    "log-vraisemblance.\n",
    "\n",
    "Qu'est-ce que vous observez?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.73291139 0.26708861]\n",
      " [0.99527187 0.00472813]]\n",
      "[[0.5682243  0.4317757 ]\n",
      " [0.49624866 0.50375134]]\n"
     ]
    }
   ],
   "source": [
    "######part plusieurs fois (100x) d'une initialisation aléatoire des paramètres de l'HMM ????\n",
    "##faire 100 itérations avec viterbi train avec params initiaux différent\n",
    "\n",
    "\n",
    "# Viterbi Training  deuxiemme version\n",
    "def viterbi_train2(sample, pi, trans, emiss, max_iter = 100):\n",
    "    \n",
    "    #first iter to get a log likelihood\n",
    "    path, devnull = viterbi(sample, pi, trans, emiss)\n",
    "    best_log = log_likelihood(sample, path, pi, trans, emiss)\n",
    "    \n",
    "    path = np.array(list(map(int, path)))\n",
    "    Pij = estimate(path)\n",
    "    Eij = estimate(sample) # how is it useful ? The sample won't and shouldn't be modified.     \n",
    "    best_param = [Pij, Eij]\n",
    "    \n",
    "    #iteration\n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        pi0, Pij, Eij = rand_param()\n",
    "        Pij, Eij = viterbi_train(sample, pi0, Pij, Eij)\n",
    "        path, devnull = viterbi(sample, pi, Pij, Eij)\n",
    "        like = log_likelihood(sample, path, pi, Pij, Eij)\n",
    "                \n",
    "        if(like > best_log):\n",
    "            best_log = like\n",
    "            best_param = [Pij, Eij]\n",
    "            \n",
    "    return best_param[0], best_param[1]\n",
    "\n",
    "pi0, Pij, Eij = rand_param()\n",
    "Pij, Eij = viterbi_train2(sample, pi0, Pij, Eij)\n",
    "print()\n",
    "print(Pij)\n",
    "print(Eij)\n",
    "\n",
    "#on a les mêmes valeurs de façon constante cette fois. \n",
    "#mais on s'est un peu éloigné des valeurs de counting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
