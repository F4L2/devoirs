{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Statistique en Bioinformatique : </b> TME5 </h1>\n",
    "<br>\n",
    "L’objectif de ce TME est: \n",
    "<br>\n",
    "<ul>\n",
    "<li> implémenter l'algorithme de Viterbi et l'estimation des paramèetres (en utilisant le Viterbi training)\n",
    "pour l'exemple du occasionally dishonest casino.   </li> \n",
    "</ul>\n",
    "<br>\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n",
    "<p>**Soumission**</p>\n",
    "<ul>\n",
    "<li>Renomer le fichier TME5_subject_st.ipynb pour NomEtudiant1_NomEtudiant2.ipynb </li>\n",
    "<li>Envoyer par email à edoardo.sarti@upmc.fr, l’objet du email sera [SBAS-2019] TME5 (deadline 19/03/2018 23:59)</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nom etudiant 1 :\n",
    "<br>\n",
    "Nom etudiant 2 :\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Introduction</h3>\n",
    "Un casino parfois malhonnête (occasionally dishonest casino) utilise 2 types de pieces : fair et unfair. <br>\n",
    "La matrice de transition entre les états cachés est:<br>\n",
    "${\\cal S}=\\{F,U\\}$ (fair, unfair):\n",
    "$$\n",
    "p = \\left(\n",
    "\\begin{array}{cc}\n",
    "0.99 & 0.01\\\\\n",
    "0.05 & 0.95\n",
    "\\end{array}\n",
    "\\right)\\ ,\n",
    "$$\n",
    "\n",
    "les probabilités d'éemission des symboles \n",
    "${\\cal O} = \\{H,T\\}$ (head, tail):\n",
    "\\begin{eqnarray}\n",
    "e_F(H) =  0.5 &\\ \\ \\ \\ &\n",
    "e_F(T) = 0.5 \\nonumber\\\\\n",
    "e_U(H) = 0.9 &\\ \\ \\ \\ &\n",
    "e_U(T) = 0.1 \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "<br> Et la condition initiale $\\pi^{(0)} = (1,0)$ (le jeux commence toujours avec le pieces juste (fair)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercice 1</b>:\n",
    "<u>Simulation</u>: Ecrire une fonction qui simule $T$ jets de pieces. \n",
    "La fonction renverra un tableau à deux colonnes correspondant \n",
    "aux valeurs simulées pour les états cachés $X_t$ \n",
    "(type de dés utilisée, “F” ou “U”) et aux symboles observées $Y_t$ \n",
    "(résultat du jet de dés, “H” ou “T”). On simulera une séquence\n",
    "de longueur 2000 qu'on gardera pour les applications ultérieures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "S = { 0:'F',1 :'U'}\n",
    "Pij = np.array([[0.99,0.01], [0.05,0.95]])\n",
    "\n",
    "O = {0:'H', 1: 'T'}\n",
    "Eij = np.array([[0.5,0.5], [0.9,0.1]]) #ça aurait dû être Eio\n",
    "\n",
    "# Condition initiale\n",
    "pi0=np.array([0.999,0.001])\n",
    "\n",
    "T = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Fonction qui simule T jets de pieces\n",
    "def jets(T, pi0, Eij, Pij):\n",
    "    # Creation du tableau\n",
    "    jetsRes = np.zeros((T,len(pi0)),dtype=int)\n",
    "    \n",
    "    ffRange = Pij[0][0]\n",
    "    fuRange = ffRange + Pij[0][1]\n",
    "        \n",
    "    ufRange = Pij[1][0]\n",
    "    uuRange = ufRange + Pij[1][1]\n",
    "    \n",
    "    eFHRange = Eij[0][0]\n",
    "    eFTRange = eFHRange + Eij[0][1]\n",
    "    \n",
    "    eUHRange = Eij[1][0]\n",
    "    eUTRange = eUHRange + Eij[1][1]\n",
    "\n",
    "    jetsRes[0][0] = 0\n",
    "    uniRandNum2 = random.uniform(0,1)\n",
    "    if(uniRandNum2 <= eFHRange):\n",
    "        jetsRes[0][1] = 0\n",
    "    else:\n",
    "        jetsRes[0][1] = 1\n",
    "        \n",
    "    for t in range(1,T):\n",
    "        if(jetsRes[t-1][0] == 0):\n",
    "            trajProb(jetsRes, t, eFHRange, eUHRange, ffRange)\n",
    "        else:\n",
    "            trajProb(jetsRes, t, eFHRange, eUHRange, ufRange)\n",
    "    return jetsRes\n",
    "\n",
    "\n",
    "def trajProb(jetsRes, t, eFHRange, eUHRange, rang):\n",
    "    uniRandNum1 = random.uniform(0,1)\n",
    "    #print(uniRandNum1)\n",
    "    if(uniRandNum1 <= rang):\n",
    "        jetsRes[t][0] = 0 \n",
    "        uniRandNum2 = random.uniform(0,1)\n",
    "        if(uniRandNum2 <= eFHRange):\n",
    "            jetsRes[t][1] = 0\n",
    "        else:\n",
    "            jetsRes[t][1] = 1\n",
    "    else:\n",
    "        jetsRes[t][0] = 1\n",
    "        uniRandNum2 = random.uniform(0,1)\n",
    "        if(uniRandNum2 <= eUHRange):\n",
    "            jetsRes[t][1] = 0\n",
    "        else:\n",
    "            jetsRes[t][1] = 1\n",
    "\n",
    "def imprimerResultats(resultat):\n",
    "    for i in resultat : \n",
    "        print (S[i[0]], O[i[1]])\n",
    "\n",
    "jetsRes = jets(T, pi0, Eij, Pij)\n",
    "imprimerResultats(jetsRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercice 2</b>: <u>Algorithme de Viterbi </u>: Ecrire une fonction qui permet\n",
    "de déterminer la séquence $(i^\\star_t)_{t=0:T}$ d'états cachés\n",
    "plus probable, ansi que sa probabilité. Pour tester votre fonction utiliser le résultat de la \n",
    "simulation (2éme colonne) de la question 1. Comparer $(i^\\star_t)_{t=0:T}$ avec\n",
    "les vrais états cachés (1ère colonne de la simulation). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithme de Viterbi\n",
    "import operator\n",
    "\n",
    "def viterbi(jets,P,E,pi,enLog):\n",
    "    \n",
    "    obs = jets[:,1]\n",
    "    \n",
    "    nS = len(P) #Nombre d'états\n",
    "    T = len(obs) #Nombre d'observations (longueur des observations)\n",
    "    Delta = np.zeros((nS,T))\n",
    "    Psi = np.zeros((nS,T)) # Selon Rabiner, on définit une vecteur Psi qui nous aide dans le processus de traceback!\n",
    "    i_star = np.zeros((T))\n",
    "    for i in range(0,nS):\n",
    "        if(enLog):\n",
    "            Delta[i][0] = np.log(pi[i]) + np.log(E[i][obs[0]])\n",
    "            Psi[i][0] = -1\n",
    "        else:\n",
    "            Delta[i][0] = pi[i] * E[i][obs[0]]\n",
    "            Psi[i][0] = 0\n",
    "        \n",
    "    for t in range(1,T):\n",
    "        for j in range(0,nS):\n",
    "            if(enLog):\n",
    "                v = Delta[:,t-1] + np.log(P[:,j])\n",
    "            else:\n",
    "                v = Delta[:,t-1] * P[:,j]\n",
    "            i, maxvalue = max(enumerate(v), key=operator.itemgetter(1))\n",
    "            \n",
    "            if(enLog):\n",
    "                Delta[j][t] = maxvalue + np.log(E[j][obs[t]])\n",
    "            else:\n",
    "                Delta[j][t] = maxvalue * E[j][obs[t]]\n",
    "            Psi[j][t] = i\n",
    "    \n",
    "    v = Delta[:,T-1]\n",
    "    i_star[T-1], prob = max(enumerate(v), key=operator.itemgetter(1))\n",
    "    \n",
    "    for t in range(T-2,0,-1):\n",
    "        i_star[t] = Psi[int(i_star[t+1])][t+1]\n",
    "    \n",
    "    return i_star, prob\n",
    "\n",
    "def analyseResultats(jets, estimation):    \n",
    "    diff = jets[:,0]-estimation    \n",
    "    error = (np.count_nonzero(diff)*100) / len(jets)\n",
    "    return error\n",
    "\n",
    "i_est, p_est = viterbi(jetsRes,Pij,Eij,pi0,False)\n",
    "error = analyseResultats(jetsRes, i_est)\n",
    "print('erreur d\\'estimation de viterbi:')\n",
    "print(error,'%')\n",
    "#print('Probabilité estimé:')\n",
    "#print(p_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercice 3</b>: <u>Estimation des paramètres</u>\n",
    "<br>\n",
    "3.1) Ecrire une fonction qui utilise tous les résultats de la simulation\n",
    "(états et symboles) pour compter les nombres d'occurrence $N_{ij}$ est $M_{iO}$ définis\n",
    "en cours. Estimer $p_{ij}$ est $e_i(O)$, voir slides  37-39 dans la presentation. Attention, pour eviter les probabilites à zero nous alons utiliser les pseudo-count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation de Parametres par contage\n",
    "def nombresOccurrence(jets,nS,nO):\n",
    "    \n",
    "    etat_seq = jets[:,0]\n",
    "    #nS = len(set(etat_seq)) #Nombre d'états\n",
    "    obs_seq = jets[:,1]\n",
    "    #nO = len(set(obs_seq)) #Nombre d'observations possibles\n",
    "    T = len(obs_seq) #Nombre d'observations (longueur des observations)\n",
    "    \n",
    "    Nij = np.ones((nS,nS)) #pseudo-count = 1\n",
    "    Mio = np.ones((nS,nO))  #pseudo-count = 1\n",
    "        \n",
    "    for i in range(0,T-1): \n",
    "        Nij[etat_seq[i]][etat_seq[i+1]] += 1\n",
    "        Mio[etat_seq[i]][obs_seq[i]] += 1\n",
    "    \n",
    "    pi = np.ones((nS)) / 1000 # Il n'y a pas assez d'informations pour π\n",
    "    pi[etat_seq[0]] = 0.999  # On doit répéter le procès de génération de séquence de plusieurs fois pour obtenir le meilleur π\n",
    "        \n",
    "    # normalisation\n",
    "    for n in range(0,nS):\n",
    "        Nij[n] = Nij[n] / sum(Nij[n])\n",
    "        Mio[n] = Mio[n] / sum(Mio[n])\n",
    "    pi = pi/pi.sum()\n",
    "    \n",
    "    return Nij,Mio,pi\n",
    "\n",
    "Nij,Mio,pi = nombresOccurrence(jetsRes,2,2)\n",
    "\n",
    "print('Nij estimé:')\n",
    "print(Nij)\n",
    "print('\\nMio estimé:')\n",
    "print(Mio)\n",
    "print('\\npi0 estimé:')\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2) <u> Viterbi training </u>: Ecrire une fonction qui utilise \n",
    "seulement la séquence $(O_t)_{t=0:T}$ (2emme colone de la simulation) pour estimer les \n",
    "paramètres $p_{ij}$ est $e_i(O)$. On s'arretera quand les diferences entre les logVraissamblance est inferieur à 1e-04. Comparer les résultats de 3.1 et de 3.2 (3.2 avec plusieurs restarts,\n",
    "et avec initialisation des paramètres alèatoire).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialisation aleatoire de Pij, Eij, pi0\n",
    "def InititRandom(nS,nO):\n",
    "    random.seed(10)\n",
    "    Pij_init = np.random.uniform(0,1,(nS,nS))\n",
    "    Eij_init = np.random.uniform(0,1,(nS,nO))\n",
    "    pi0_init = np.random.uniform(0,1,nS)\n",
    "    return Pij_init,Eij_init,pi0_init\n",
    "\n",
    "# Calcule log Vraissamblance\n",
    "def logLikelhihood(Aij,Bij,pi,jets):\n",
    "    etat_seq = jets[:,0]\n",
    "    obs_seq = jets[:,1]\n",
    "    T = len(obs_seq) #Nombre d'observations (longueur des observations)\n",
    "    lLikelihood = np.log(pi[etat_seq[0]])\n",
    "    lLikelihood += np.log(Bij[etat_seq[0]][obs_seq[0]])\n",
    "    for i in range(1,T):\n",
    "        lLikelihood += np.log(Aij[etat_seq[i-1]][etat_seq[i]]) + np.log(Bij[etat_seq[i]][obs_seq[i]])\n",
    "    return lLikelihood\n",
    "\n",
    "# Viterbi Training\n",
    "def Training(jets):\n",
    "    jets_est = np.array(jets)    \n",
    "    nS = 2 #Nombre d'états\n",
    "    nO = 2 #Nombre d'observations possibles\n",
    "    Pij_est,Eij_est,pi0_est = InititRandom(nS,nO)\n",
    "    \n",
    "    nIteration = 10000\n",
    "    iCount = 0\n",
    "    criterion = 1e-04\n",
    "    lLikelihood = np.zeros((nIteration))\n",
    "    while(iCount < nIteration):\n",
    "        i_est, p_est = viterbi(jets_est,Pij_est,Eij_est,pi0_est,False)\n",
    "        jets_est[:,0] = i_est\n",
    "                \n",
    "        Pij_est,Eij_est,pi0_est = nombresOccurrence(jets_est,nS,nO)\n",
    "        \n",
    "        lLikelihood[iCount] = logLikelhihood(Pij_est,Eij_est,pi0_est,jets_est)\n",
    "        \n",
    "        if(iCount > 0):\n",
    "            if(abs(lLikelihood[iCount]-lLikelihood[iCount-1]) <= criterion):\n",
    "                break\n",
    "        iCount+=1\n",
    "        \n",
    "    lLikelihood = lLikelihood[:np.argmax(lLikelihood)]\n",
    "    return Pij_est,Eij_est,pi0_est,lLikelihood\n",
    "    \n",
    "#imprimer les Parametres du Viterbi Training\n",
    "Pij_est,Eij_est,pi0_est,lLikelihood = Training(jetsRes)\n",
    "itCount = len(lLikelihood)\n",
    "print('Le modèle est convergé après '+str(itCount)+' itérations.')\n",
    "print('\\nPij estimée:')\n",
    "print(Pij_est)\n",
    "print('\\nEij estimée:')\n",
    "print(Eij_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "Ici, on a remarqué que parfois le processus d'apprentissage converge vers un minimum local (mauvais modèle). On dois exécuter la fonction d'apprentissage plusieurs fois pour trouver le meilleur modèle. Ce problème est résolu dans la section suivante.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3) <u>Viterbi training deuxiemme version </u> Ecrivez une version de 3.3 qui:\n",
    "- part plusieurs fois (100x) d'une initialisation aléatoire des \n",
    "paramètres de l'HMM,\n",
    "- utilise Viterbi training pour estimer les paramètres,\n",
    "- calcule la log-vraisemblance pour les paramètres estimés,\n",
    "- sauvegarde seulement l'estimation avec la valeur maximale de la\n",
    "log-vraisemblance.\n",
    "\n",
    "Qu'est-ce que vous observez?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Training  deuxiemme version\n",
    "def TrainingV2(jets,nIterat):\n",
    "    itCount = np.zeros((nIterat)) #Pour savoir chaque fois le nombre d'itérations nécessaires pour converger\n",
    "    Pij_meilleur = []\n",
    "    Eij_meilleur = []\n",
    "    pi0_meilleur = []\n",
    "    lLikelihood_meilleur = -10000\n",
    "    for i in range(0,nIterat):\n",
    "        Pij_est,Eij_est,pi0_est,lLikelihood = Training(jetsRes)\n",
    "        \n",
    "        itCount[i] = len(lLikelihood)\n",
    "        lastlLikelihood = lLikelihood[-1]\n",
    "        \n",
    "        if(lastlLikelihood > lLikelihood_meilleur):\n",
    "            lLikelihood_meilleur = lastlLikelihood\n",
    "            Pij_meilleur = Pij_est\n",
    "            Eij_meilleur = Eij_est\n",
    "            pi0_meilleur = pi0_est\n",
    "    return Pij_meilleur, Eij_meilleur, pi0_meilleur, lLikelihood_meilleur, itCount\n",
    "    \n",
    "\n",
    "# Imprimer les Parametres du Viterbi Training deuxiemme version\n",
    "nIterat = 100\n",
    "Pij_meilleur, Eij_meilleur, pi0_meilleur, lLikelihood_meilleur, itCount = TrainingV2(jetsRes,nIterat)\n",
    "\n",
    "print('Meilleur Pij estimée:')\n",
    "print(Pij_meilleur)\n",
    "print('\\nMeilleur Eij estimée:')\n",
    "print(Eij_meilleur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "Les valeurs initiales des matrices transition et émission est très importante pour le processus d'apprentissage; parce que certaines des valeurs initiales pourraient mener à un minimum local des fonctions de coût (l'apprentissage s'arrête au minimum local au lieu du minimum global). C'est pourquoi dans cet exercice on avait répété l'apprentissage avec des valeurs initiales différentes pour trouver le meilleur modèle.  \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem(itCount)\n",
    "plt.title('Nombre d\\'itérations nécessaires pour converger chaque fois!')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
